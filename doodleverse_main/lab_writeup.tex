\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{advdate} %other dates than today
\usepackage{sectsty} % Allows customizing section commands
%\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{geometry}
\usepackage{listings} %allows to print code
\usepackage[per-mode=symbol]{siunitx} %units (hertz)
\usepackage{graphicx} %allows images
\usepackage{epstopdf} %enables eps in pdfs
\usepackage{float} %fixes images
\usepackage{caption} %allows changing captions
\usepackage{polynom} %polynomial long division
%hobbled together long division?
\usepackage[display]{texpower}
%\usepackage{titlesec}
%\usepackage{hyperref}


\newcommand{\ldsym}{$\left.\mathstrut\right)$}% unbalanced )
\newlength{\ldwidth}

\newcommand{\longdivide}[2]% #1 = denominator, #2 = numerator
{\settowidth{\ldwidth}{\ldsym}
$#1\,\raisebox{1.5pt}{\ldsym}\hspace*{-.65\ldwidth}\overline{
\mathstrut\hspace*{.35\ldwidth}\ #2}$}

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{0pt} % Customize the height of the header

\setlength\parindent{0pt} % Removes all indentation from paragraphs

\setlength{\parskip}{.5cm} % space between paragraphs

%margins
\geometry{a4paper, top=1in, left=1in, right=1in, bottom=1.4in, includehead, includefoot}

%Don't number sections
%\renewcommand{\thesection}

%Don't bold section headings
%\sectionfont{}
%\subsectionfont{\textnormal}

\lstset{basicstyle=\ttfamily\small,xleftmargin=-1cm,xrightmargin=-2cm,breaklines=true} %settings and font for code

\usepackage[utf8]{inputenc}
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{9} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{9}  % for normal
% Defining colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}


\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
backgroundcolor=\color{white},
basicstyle=\ttm,
otherkeywords={self},            
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          
emphstyle=\ttb\color{deepred},    
stringstyle=\color{deepgreen},
commentstyle=\color{red},
frame=tb,                         
showstringspaces=false            
}}

% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}
\renewcommand{\thesubsection}{\alph{subsection}} %Lettered subsections
%\renewcommand{\thesubsubsection}{\roman{subsubsection}} %lower case roman numeral subsubsections

\renewcommand{\labelenumi}{\arabic{enumi}.}


\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\DeclareMathOperator*{\argmin}{arg\,min} %argmin

\DeclareMathOperator*{\argmax}{arg\,max} %argmax

\title{
\normalfont \normalsize
\huge The DoodleVerse \\ % The assignment title
}
\author{D. Conathan, Z. Pace, J. Vahala (ECE/CS 532; Fall 2015)}
\date{December 2015}

\begin{document}
\maketitle


\begin{figure*}[!h]
\center{\includegraphics[scale=1]{space.eps}}
\captionsetup{labelformat=empty}
\caption{}
\end{figure*}

\section{Introduction}

Each civilization has their own set of constellations: groups of stars ascribed to a particular piece of cultural
memory. The ancient Greeks gave us many modern constellations such as the mythical hunter, Orion, who is best known by the stars that make his belt. But with the sheer number of stars in they sky, can we not supercede the Greeks by taking our own arbitrary doodles and then finding sets of stars that fit them? In this lab, we’ll explore how to take a set of stars and construct spectacular celestial displays of our own design. Welcome to the DoodleVerse!

\section{Overview}

The problem can be broken up into two main tasks: (1) taking the input image and extracting the ideal feature points, and (2) finding the ideal set of stars whose formation matches this set of feature points.

For the first task, we must perform a series of image processing steps to separate an image into a set of feature points. We must (a) define the image contours as sets of $k$ (x,y)-coordinates that accurately depict the original image, and (b) reduce each contour into sets of $n<k$ (x,y)-cooridinates that do not substantially degrade the original image representation. We do this by comparing the original contour to a linear spline (en.wikipedia.org/wiki/B-spline) connecting individual feature points and adding or removing points as necessary. 

For the second task, we start by framing it as an optimization problem. If $\mathbb{S}$ is our set of stars and $F$ is the matrix defined by our feature points, then we are looking for the subset $S\subset\mathbb{S}$ that minimizes the following: $||TF - S||_F$.  Here $T$ is a map from the Cartesian "feature space" to the space that our stars lie in that preserves the shape of our feature points (i.e. does not distort angles and relative distances).

The number of stars in our $\mathbb{S}$ set (which has been restricted to stars visible with the naked eye) is around 3000, so for $n$ feature points, there are around 3000 choose $n$ possibilities for $S$. The search space is too large for a brute force method.  We will proceed by breaking this problem into smaller problems to the point where a random search will yield sufficiently quick and accurate results. For this lab, we have provided a subset of $131$ stars that lie in one portion of the sky.
\section{Warmup}

\subsection{Getting Feature Points}

In order to find a set of $n$ feature points of a drawing, we first need to turn the drawing into a set of contours from which we will get our feature points. In order to accomplish this, we need some form of edge detector. 

To detect edges in an image, we can think of them as large-amplitude changes in either the x-direction, the y-direction, or both. A standard way to do this is with the Sobel operator, which convolves an image with two $n$ by $n$ kernels, each of which is designed to detect changes in either the x or y axis (en.wikipedia.org/wiki/Sobel\_operator). The general form of the result of applying this method to some matrix $A$ is $G = \sqrt{G_x^2 + G_y^2}$, where $G_x = \begin{bmatrix}
-1& 0 & 1\\
-2 & 0& 2\\
-1 & 0& 1
\end{bmatrix}\star A$ and $G_x =\begin{bmatrix}
-1 & -2 & -1\\
0 & 0 & 0\\
-2 & 0 & 2
\end{bmatrix} \star A$. We can implement each of these axis-specific operators using \texttt{scipy.ndimage.filters.sobel(A, mode='constant', ...)}, summing the squares of the results for \texttt{axis=0} and \texttt{axis=1}.

A more advanced border-following technique can be used for defining continuous contours \cite{Suzuki1985}. This technique is more useful than simple edge detection techniques because it determines an ordered contour instead of points of high "edge magnitude". The use of this technique is shown in Figure \ref{feat_detect}(b).

\fbox{\begin{minipage}{40em}
\textbf{For a simple image, say a triangle with a black 1 pixel wide edge on a white background, suggest a reasonable edge detector to find the contour of the image. Why might a simple edge detector be "good enough" for this application?}
\end{minipage}}

\fbox{\begin{minipage}{40em}
\textbf{Answer:} An edge detector as simple as [-1 0 1] can be used because we aren't interested in any complicated features that may only be detected by a complex edge detector. Further, we know that a triangle can be appoximated by three points that necessarily have a horizontal edge component, so a horizontal edge detector like [-1 0 1] will be "good enough" to find the three points we are looking for. 
\end{minipage}}

Next, in order to narrow down our possible feature points, we can use a corner detector (en.wikipedia.org/wiki/Corner\_detection). Similar to how edge detectors look for large changes in particular directions, corner dectors look for large changes in more than one direction. One particular tool is the Harris Corner Detector.

Corners can be thought of as the intersection of two edges, so the gradient of the image will be large in both directions. In the case of the Harris Corner Detector, if the function $I(x, y)$ represents the intensity of an image at position $(x, y)$, then we want to maximize $J(u, v) = \sum_{x,y} w(x, y) [I(x+u, y+v) - I(x, y)]^2$, where $w(x, y)$ is the window over which we’re looking at position $(x, y)$ with $u$ and $v$ as the half-widths of the window in the $x$ and $y$ directions. After a Taylor expansion and some algebra, we can express $J$ as 

$$
J(u, v) \approx \begin{bmatrix}
u & v
\end{bmatrix} M \begin{bmatrix}
u\\
v
\end{bmatrix}
$$
where
$$
M = \sum_{x,y} w(x, y) \begin{bmatrix}
I_x^2 & I_x~I_y \\ I_x~I_y & I_y^2
\end{bmatrix}
$$

Now, for each window, a score $R$ is calculated and compared to a threshold value, to determine if a corner is present:
$$
R = [det(M) - k(tr(M)]^2
$$
Furthermore, $det(M)$ and $tr(M)$ can be expressed in terms of the eigenvalues $\lambda_1$ and $\lambda_2$: $det(M) = \lambda_1 \times \lambda_2$ and $tr(M) = \lambda_1 + \lambda_2$. In summary, the results for Harris corner detection may still vary, depending on the choice of $u$, $v$, and $R$.

The local maxima from this approach yields the corners found in the image. By thresholding the $m$ largest local-maxima corners, the original set of $k >> n$ points can be largely reduced to a more manageable set of feature points as shown in Figure \ref{feat_detect}(c). This set can be fine tuned by additional methods to find better feature points. This is necessary because these $m$ feature points are not necessarily the most representative features of the contour. Many shapes may not give features that may seem obvious to us, so it is often useful to include a set of points that will exist for any contour so that the shape will at least have a minimal representation.  

\fbox{\begin{minipage}{40em}
\textbf{Given an arbitrary two-dimensional shape, determine some candidate feature points that will always exist.}
\end{minipage}}

\fbox{\begin{minipage}{40em}
\textbf{Answer:} \\ The extreme points (top-, bottom-, left-, and right-most) will always exist.
\end{minipage}}
\\
\\
\\
\\
Finally, in order to turn our $m$ feature points into the $n$ feature points that best represent the contour, we perform an algorithm which adds and removes points according to user-defined thresholds. We determine this set of $n$ points by testing points $p_{x_i},_{y_i}  i = 1, 2, ..., m$ for how much their removal would induce error between the new contour $c_i$ and the original $k$ point contour $c^*$ by calculating the normalized absolute error $E$ between $c_i$ and $c^*$: $$E = \frac{1}{|| p_{x_{i-1},y_{i-1}}  -  p_{x_{i+1},y_{i+1}} ||_2} \int\limits_{p_{x_{i-1},y_{i-1}}}^{p_{x_{i+1},y_{i+1}}} |l(x,y)| \ dx \ dy$$ Where $l(x,y)$ is the curve of $c^*$ relative to the linear spline connecting $p_{x_{i-1},y_{i-1}}$ and $p_{x_{i+1},y_{i+1}}$. The goal thus becomes to choose only those points that, for some theshold $T$, do not induce error $E > T$. The algorithm is defined below. Note that the most important aspect is the selection of the threshold values $t_{delete}$ and $t_{add}$. 

\textbf{N-feature selection algorithm}
\\
(see Figure \ref{feat_detect}(d) for example of reducing an $m = 18$ point set to an $n = 10$ point set)
\begin{enumerate}
	\item 
	For all points $p_i$ in the set of $m$ features, find normalized error $E_i$ between $p_{i-1}$ and $p_{i+1}$. If $E_i < t_{delete}$, remove $p_i$ from the set of feature points. Else, move on to the next point until all redundant points $j$ have been removed and the new set is composed of $y = m - j$ points.  
	\item
	For all points $p_i$ in the set of $y$ features, find normalized error $E_i$ between $p_{i}$ and $p_{i+1}$. If $E_i > t_{add}$, then add a new feature point along the contour at the bisecting point and test this new point to check if it meets the $t_{add}$ criterion. Else, move on to the next point. When finished, the new set of feature points is composed of $n = y + r$ points, where $r$ is the number of points added by this step. 
\end{enumerate}

\fbox{\begin{minipage}{40em}
\textbf{With the star dataset being used in this application taken into consideration, why is it so important to represent the shape in as few feature points as possible?}
\end{minipage}}

\fbox{\begin{minipage}{40em}
\textbf{Answer:} Using every point on the border as a feature point gives very little \textit{real} information, and will make it impossible to map between those $N \sim$ 2,000 points and the $M \sim$ 3000 stars in our catalog.
\end{minipage}}

Finally, our approach that will be discussed later depends on picking three important initial feature points, so we must determine a good set of three points of the $n$ features that will give us a good chance at finding a match. This set of points will have well defined angles and large enough interpoint distances such that the three points do not line up with each other for form a line or near-line. Thus, the problem becomes a search for which three adjacent feature points along the contour best meet these requirements. In other words, which point solves the following: $p^* = max_{p_i} [ (||p_i - p_{i-1}||_2 + ||p_i - p_{i+1}||_2) sin(\theta_i) ]$ where $\theta_i$ is the angle spanning $p_{i-1}$ and $p_{i+1}$ going through $p_i$. An example of the key feature points is identified in Figure \ref{feat_detect}(d). 

\begin{figure}
\center{\includegraphics[scale=0.8]{lab_appendix_imgs/duck_features.png}}
\caption{(a) Original Image, (b) Contour of shape determined by \cite{Suzuki1985}, (c) Basic feature points determined by Harris Corner Detector, (d) Optimized feature points determined by thresholding algorithm with key feature points explicitly identified}
\label{feat_detect}
\end{figure}

\pagebreak

\subsection{Cartesian and Spherical Coordinates: The Mollweide Projection}
Next we want to begin our search to see if we can find a set of stars that fits these feature points. The first issue we encounter is that our extracted feature points have Cartesian coordinates in the $(x,y)-$plane and our star data set is in spherical coordinates. Here is a readout of the first three stars the data set we provided in $stars.mat$:
\begin{python}
	     RA          Dec           Mag       Index
	  float64      float64       float64     int32
	------------ ------------ -------------- -----
	 263.4021936 -37.10374835 0.440288769784  2096
	  262.690995 -37.29574016 0.595684453237  2088
	264.13686135 -38.63486521 0.820144884892  2100
\end{python}
The "RA" or "right ascension" measures the distance from a central meridian and ranges from $0$ hours to $24$ hours (which corresponds to $0\si{\degree}-360\si{\degree}$). We have converted hours to degrees by multiplying this column by $15$.
The "Dec" or "declination" measures the distances above or below the equator and ranges from $-90\si{\degree}$ to $90\si{\degree}$. The "Mag" or "magnitude" corresponds to the stars brightness (we have normalized the magnitudes so that they fall in the range $(0,1)$). The scale is reversed: \textit{lower numbers correspond to brighter stars}.
Index refers to a unique ID assigned to each star in our starset.

The Mollweide (or homolographic) projection is a very useful projection for plotting objects in the night’s sky. Unlike most maps we are used to seeing of the Earth’s surface, the Mollweide projection is ``equal-area”, meaning that the area enclosed within a given, closed curve is the same as the area enclosed within the Mollweide projection of that curve. Additionally, locally, the projection is flat, meaning that the un-projected data will look the same as the projected data if we zoom in far enough (realistically, this happens at scales of a few degrees). The Mollweide projection (and its cousin, the Hammer projection) have been used for about 30 years by the Cosmic Microwave Background physics community, who have used it to represent the temperature of the universe’s nearly-constant radiation field over the whole extent of the sky.  See Figure \ref{moll} for an example Mollweide projection.

\begin{figure}
\center{\includegraphics[scale=0.15]{mollweide.png}}
\caption{Mollweide projection of the temperature of the Cosmic Microwave Background Radiation (CMB), a nearly-uniform 2.7-Kelvin radiation source (Image: Wikipedia; Data: Wilkinson Microwave Anisotropy Probe)}
\label{moll}
\end{figure}


Functionally, the Mollweide projection has equally-spaced vertical parallels, but the meridians compress significantly near the poles, so that the whole unit sphere is represented as an ellipse with an axis ratio of two to one. We decided to use the Mollweide projection in our application because it reduces to very little distortion at small spacial scales (on the order of a few degrees - the maximum scale over which we will form our constellations).

The Mollweide projection of a star with spherical coordinates $(\lambda,\phi)$ centered around ($\lambda_c,\phi_c$) can be obtained as follows \cite{Snyder1987}:
$$
x = R\frac{2\sqrt{2}}{\pi}(\lambda-\lambda_c)cos(\theta)$$$$
y = R\sqrt{2}sin(\theta)
$$
Where $\theta$ is the angle defined by: $2\theta + sin(2\theta)=\pi sin(\phi-\phi_c)$.  Since $\theta$ is implicitly defined, we cannot solve for it directly. But the following iteration will converge to its value after a few iterations (it converges slow for points far from our center point, but that is not a concern for us).
$$
\theta_0 = \phi-\phi_c
$$$$
\theta_{k+1} = \theta_k + \frac{2\theta_k+sin(2\theta_k)-\pi(sin\phi-\phi_c)}{2+2cos(2\theta_k)}
$$

\pagebreak

Here is a Python script which will compute the Mollweide projection for given spherical coordinates:
\begin{python}

def project(ra, dec, c_ra, c_dec):
	'''
	Finds the Mollweide projection coordinates (x,y) for the point (ra,dec) around 
	point (c_ra,c_dec).
	'''
	
	# Find theta
	theta_0 = dec - c_dec
	epsilon = 10**-6
	error = 1+epsilon
	
	while error > epsilon:
	    m = (2*theta_0+np.sin(2*theta_0)-np.pi*np.sin(dec - c_dec))/(2+2*np.cos(2*theta_0))
	    theta_1 = theta_0 - m
	    error = np.abs(theta_1 - theta_0)
	    theta_0 = theta_1
	
	# Compute (x,y) coordinates
	x = 2*np.sqrt(2)*(ra-c_ra)*np.cos(theta_0)/np.pi
	y = np.sqrt(2)*np.sin(theta_0)
	
	return [x,y]
\end{python}

\fbox{\begin{minipage}{40em}
\textbf{Use this script to convert the 79 stars found in $stars.mat$ to (x,y) coordinates centered at ($\lambda_c,\phi_c)=(263, -37)$. (we have converted hours to degrees for you, but be sure to convert degrees to radians for the funcion defined above)}
\end{minipage}}

\fbox{\begin{minipage}{40em}
\textbf{Answer:} Answers will vary, but the results should look like the figure.
\end{minipage}}

\begin{figure}[!h]
\center{\includegraphics[scale=.7]{projection.eps}}
%\captionsetup{labelformat=empty}
\caption{Example projection result.}
\end{figure}

\pagebreak

\subsection{Preliminary Search: A Clustering Approach}

We wish to determine if there is possible good match for our feature points in the set obtained from the previous section.  Since we are only trying to match the shape and not the scale of our feature points, one approach is to find a set of stars that forms the same set of interior angles.

In essence, this is a clustering problem. First, let's simplify the problem by considering the case where our feature points consist of just 3 points: $(p_1,p_2,p_3)$. The shape of these 3 points is uniquely determined by their interior angles (i.e. the angles of the triangle they form), call them $(a_1,a_2,a_3)$. Finding these angles is a simple geometric formula such that the angle formed by $p_1, p_2, p_3$ with a vertex at $p_1$ is given by:

$$\arccos\left(\frac{P_{12}^TP_{13}}{||P_{12}||||P_{13}||}\right)$$

Where $P_{ij}$ is the vector formed by $(p_i - p_j)$.

\pagebreak
Here is the Python code that implements this:
\begin{python}
    def GetAngles(verts):
        '''
        Get angles between vertices specified by first 3 points in verts
        '''
        d = np.zeros((3))
        ab = verts[0]-verts[1]
        ac = verts[0]-verts[2]
        cb = verts[1]-verts[2]
        d[0] = np.linalg.norm(ab)
        d[1] = np.linalg.norm(ac)
        d[2] = np.linalg.norm(cb)
        
        a = np.zeros((3))
        a[0] = np.arccos(np.dot(ab,ac)/(d[0]*d[1]))
        a[1] = np.arccos(np.dot(ab,cb)/(d[0]*d[2]))
        a[2] = np.arccos(np.dot(ac,cb)/(d[1]*d[2]))
        
        return a
\end{python}


If we think of every set of 3 points in our search space as a specified by their interior angles (sorting the angles in ascending order uniquely determines the set's "angle representation"), we are basically looking for the 3-point sets that lie in the same cluster as our feature points. This cluster would be our "possible match" cluster.

This is very valuable when searching over our entire search space: it effectively finds all possible matches for a given triangle. 

\fbox{\begin{minipage}{40em}
\textbf{Consider this as the clustering problem $X = DW$. What do $X,D,W$ look like? Our search set contains 131 stars. If W is $k\times n$, what is $n$? What would be the effect of choosing smaller or larger values of $k$?}
\end{minipage}
}

\fbox{\begin{minipage}{40em}
\textbf{Answer: }$X$ is a $3\times n$ matrix where each column contains in the interior angles of each triangle. $D$ is $3\times k$ and each row contains the coordinates of the cluster centers, and there are $k$ clusters. $n$ would be 131 choose 3 = 366,145. $W$ is $k\times n$ and for each column contains all 0s except a 1 to indicate that triangle is in that cluster.  Choosing larger values of $k$ would increase the computation time and decrease the average number of triangles per cluster.
\end{minipage}}

The set of all possible triangles formed by our set of stars is far too large to use this clustering approach directly. Fortunately, by only considering a subset of brighter stars and limiting the size of our triangles, we can reduce the set of triangles to about $40,000$, which we can then cluster into about $1200$ clusters so that each cluster contains on average $30-40$ triangles.  This computation is time-consuming but we only need to do it once - not for each search.

\fbox{\begin{minipage}{40em}\textbf{An alternative to this clustering approach would be to just find the $k$ closest triangles to our target triangle, where $k$ is determined ahead of time.  What are the advantages of the clustering approach?
}
\end{minipage}}

\fbox{\begin{minipage}{40em}\textbf{Answer: }If there are many good matches and we choose a small $k$, we end up ignoring many of the good matches. Conversely, if there are only a few matches, choosing a large $k$ makes us consider bad matches. Clustering effectively selects a good value of $k$ for us.
\end{minipage}}

Unfortunately, this approach doesn't scale well when our feature set is more than 3 points (most notably, the search space becomes way too large). However, matching 3 feature points to 3 stars does uniquely determine a transformation from the $(x,y)-$plane that the features lie in to the $(x,y)-$plane that the stars lie in. So rather than finding a match for the remaining points using this clustering approach, we can simply check to see if there are stars in the appropriate places to form the rest of our shape. To do this, we need to find the transformation from the $(x,y)-$coordinates of our features to the $(x,y)-$coordinates of our stars. To find this transformation, we must solve what is called the Procrustes problem.


\subsection{The Orthogonal Procrustes Problem\cite{Everson1997}}
Here we address the problem of finding the ideal transformation $T$ between our set of feature points $F$ and a predetermined subset of stars $S\subset \mathbb{S}$. Here we are assuming $F$ and $S$ are the same size $(2\times k)$ and represent their respective collections of points in Cartesian coordinates (each column is a point).  Finding this transformation will be necessary to evaluate the fitness of a possible match.

Other than minimizing the distance between points, the primary goal is preserving the shape of $F$. For now, let's also constrain the problem to where $S$ and $F$ are normalized, so we don't need to change the scale of $F$: our transformation will preserve distances and angles between points (and thus the shape). This simplifies the problem because it constrains $T$ to be orthogonal. Also, let's assume that both sets of points are centered around the origin (i.e. their mean is $\begin{bmatrix}
0\\
0
\end{bmatrix}$). Basically, we are looking for the best rotation of $F$ that minimizes the distance between its points and the points in $S$. Precisely, we are looking for the orthogonal $T$ that minimizes: 
$$||TF-S||^2_F = \langle TF-S, TF-S\rangle_F$$
Where $tr(AB^T) = \langle A,B\rangle_F = \langle B,A\rangle_F = tr(BA^T)$.

\fbox{\begin{minipage}{40em}\textbf{
Let $U\Sigma V^T$ be the SVD of the matrix $SF^T$. Show that the orthogonal $T$ that minimizes $||TF-S||^2_F$ is equal to $UV^T$.}\\
Hint: $T,U$, and $V$ are orthogonal, and multiplying a vector by an orthogonal matrix doesn't change its Frobenius norm.
Hint 2: If you are really stuck, consult \cite{Everson1997}.
\end{minipage}}

\fbox{\begin{minipage}{40em}
\textbf{Answer:} \\
By the property of the trace, 

$$ T^* = \argmin_T ||T F - S||_F^2 $$

$$ T^* = \argmin_T~tr((T F-S)(T F-S)^T)$$

$$ T^* = \argmax_T~tr(S F^T T^T) $$

$$ T^* = \argmax_T~tr(U \Sigma V^T T^T) $$

$$ T^* = \argmax_T~tr(U^T T V \Sigma) $$

$U^T T V$ is orthogonal, so $tr(U^T T V \Sigma) \le tr(\Sigma)$. This is equality when $U^T T V = I$, or $T=UV^T$. So $T^*=U V^T$ is a minimizer.

\end{minipage}}

Here is the Python code that solves the orthogonal Procrustes problem:

\begin{python}
def Procrustes(F, S):
	'''
	Given F, S (same size, unit norm, and centered around the origin), finds the
	ideal transformation that maps the points of F to best fit the points of S.
	That is, ||TF-S|| is minimized.
	'''
	
	[U,s,VT] = np.linalg.svd(np.dot(S,F.T))
	
	T = np.dot(U,VT)
	
	return T
\end{python}

\fbox{\begin{minipage}{40em}
\textbf{Use the Procrustes algorithm to find the transformation that best maps the $A$ to $B$ matrix found in the $procrustes.mat$ file.}
\end{minipage}
}

\fbox{\begin{minipage}{40em}
\textbf{Answer:}
See $procrustes\_solution.py$
\end{minipage}
}
\pagebreak
\subsection{Evaluating the Match}

Now we have sufficient machinery to evaluate our possible match.  We know we have a decent match for three of our feature points, and we know how to transform these feature points so they best align with the match. We just have to determine if there is decent match for the remaining feature points. To do this, we apply the same transformation to these feature points, and search for bright stars nearby the transformed points. In the simplest case, we could just pick the closest star to each transformed point and implement some cost function that penalizes dim stars.

Now we have our set of feature points $[f_1,\ldots,f_n]=F$, our possible match set of stars $[s_1,\ldots,s_n] = S$, with $[m_1,\ldots,m_n] = M$ being the set of magnitudes for the stars in $S$.

\fbox{\begin{minipage}{40em}\textbf{Suppose we had $k$ such possible matches $\{S_i,M_i\}$, and we want to determine the \textit{best} one. Set up a cost/penalizer function that we are trying to minimize.}
\end{minipage}}

\fbox{\begin{minipage}{40em}\textbf{Answer:}
Many possible answers (depending on what norms you pick), but one possibility is:
$$\sum_{i=1}^{n}||T(f_i)-s_i||+\lambda|m_i|$$
Where $\lambda$ is the weight factor for the magnitudes $m_i$, and $T$ is the Procrustes transformation that best fits $F$ with $S$.

\end{minipage}}


\section{Lab}
You have been supplied: diamond.png (see Figure \ref{diamond}) and a subset of our star dataset $stars.mat$. Perform the following: 

\begin{figure}[!h]
\center{\includegraphics[scale=0.5]{diamond.png}}
\caption{diamond.png}
\label{diamond}
\end{figure}

\begin{enumerate}
  \item
  Import diamond.png to your program of choice (MATLAB or Python). Implement the edge detector of your choice to get a set of $k$ feature points that represent the shape. 
  \item 
  Implement a methodology of your choosing to reduce this set of $k$ feature points to $n << k$  points. How well do your features match the original image?

 \fbox{\begin{minipage}{40em}
\textbf{Answer:}
See $DoodleVerse\_Lab\_q1q2sol.m$ for solution to Lab questions 1 and 2.
\end{minipage}
}
  \item
    Pick $3$ of your $n$ points and compute the angles between them and arrange them is ascending order. Use the $clustercenters.mat$ file to determine the index of the 3 closest clusters to your triangle (by finding the 3 closest ones).
    
    The $starlabels.mat$ file contains the triangle data. For every triangle, it contains the index of each star and the label of each cluster. Use this to find the possible triangle matches that lie in the subset we provided you in $stars.mat$ (you should find $4-5$ depending on which points you started with).  \textbf{Note: the indices in the "label" variable corresponds to Python indices and range 0-1232. If you are using Matlab and looking for the row of "clustercenter" that corresponds to that index, add 1. e.g. cluster label "5" is the cluster centered at row 6 of clusterdata.}
  \item
  Generalize the orthogonal procrustes problem so that it will be useful for this case. That is, given the $(2\times 3)$ matrix of feature points $F$ and a  $(2\times 3)$ matrix of star points $S$ (which are not unit norm or centered around the origin), find ideal transformation $T$ such that $||T(F)-S||_F$ is minimized.  Note that it is easy to reduce to the previous case by computing the following:
  $$
  \hat{F} = \frac{F_c}{||F||_F} \quad\textrm{ and }\quad\hat{S} = \frac{S_c}{||S||_F}
  $$
  and solving the problem for $\hat{F}$ and $\hat{S}$, where $F_c$ is the matrix $F$ with its points centered around the origin: that is for each column, subtract $\begin{bmatrix}
mean(x \textrm{ coordinates of } F)\\
mean(y \textrm{ coordinates of } F)
  \end{bmatrix}$. Modify the previous Procrustes problem code to solve the general Procrustes problem, and for each of your possible matches found in (3), use it to map all of your $n$ feature points to the subspace containing the star projections such that your triangle of feature points best fit with the match.
  \item
  For the feature points that weren't part of your triangle, find the closest stars to their transformed positions. Repeat this process for your other good matches and evaluate the overall fit of your matches using your cost function from the previous section. Which match is best? If your cost function had a weighted penalizer for the star magnitude, how does changing this weight affect your evaluation?
\end{enumerate}

\fbox{\begin{minipage}{40em}
\textbf{Answer:}
See $lab\_solution.py$
(for the last question, making the star magnitude weight higher causes constellations with brighter stars but a worse fit to be picked)
\end{minipage}
}

\bibliography{cited_papers}
\bibliographystyle{ieeetr}

\section*{Appendix: Results}
Below we have provided results for a few sample images by applying this approach to all matches found by the initial cluster approach (which is usually around $90-200$). Note that magnitude 3 brightness is visible from most populated areas (not city centers), and magnitude 4 brightness is visible from the suburbs (and cooperating Madison nights). 

\begin{figure}[!h]
\center{\includegraphics[scale=0.5]{lab_appendix_imgs/duck_features.png}}
\caption{(a) Original, (b) Contour, (c) Harris Corner features, (d) Optimized features}
\label{duck}
\end{figure}

\begin{figure}[!h]
\center{\includegraphics[scale=0.5]{lab_appendix_imgs/duck_starset.png}}
\caption{Starset found for features of Figure \ref{duck}, avg Magnitude $\approx$ 3.2. }
\label{duckstar}
\end{figure}

\begin{figure}[!h]
\center{\includegraphics[scale=0.5]{lab_appendix_imgs/tri_features.png}}
\caption{(a) Original, (b) Contour, (c) Harris Corner features, (d) Optimized features}
\label{tri}
\end{figure}

\begin{figure}[!h]
\center{\includegraphics[scale=0.5]{lab_appendix_imgs/tri_starset.png}}
\caption{Starset found for features of Figure \ref{tri}, avg Magnitude $\approx$ 2.8. }
\label{tristar}
\end{figure}

\begin{figure}[!h]
\center{\includegraphics[scale=0.5]{lab_appendix_imgs/sword_features.png}}
\caption{(a) Original, (b) Contour, (c) Harris Corner features, (d) Optimized features}
\label{sword}
\end{figure}

\begin{figure}[!h]
\center{\includegraphics[scale=0.5]{lab_appendix_imgs/sword_starset.png}}
\caption{Starset found for features of Figure \ref{sword}, avg Magnitude $\approx$ 3.4. }
\label{swordss}
\end{figure}

\begin{figure}[!h]
\center{\includegraphics[scale=0.5]{lab_appendix_imgs/mj_features.png}}
\caption{(a) Original, (b) Contour, (c) Harris Corner features, (d) Optimized features}
\label{mj}
\end{figure}

\begin{figure}[!h]
\center{\includegraphics[scale=0.5]{lab_appendix_imgs/mj_starset.png}}
\caption{Starset found for features of Figure \ref{mj}, avg Magnitude $\approx$ 3.2}
\label{mjss}
\end{figure}

\end{document}
